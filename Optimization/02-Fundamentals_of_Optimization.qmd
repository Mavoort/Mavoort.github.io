---
title: "Introduction"
author: "Marcel Angenvoort"
date: 2025-07-01
#doi: 10.5555/12345678
bibliography: "references.bib"
abstract: >
  Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.
keywords: ["math", "optimization", "julia", "90-01", "90C30"]
jupyter: julia-1.11
execute:
  enabled: false
format: 
  html: 
    output-file: fundamentals_of_optimization.html
  revealjs:
    output-file: fundamentals_of_optimization-slides.html
#  typst:
#    output-file: fundamentals_of_optimization.pdf
---


TODO:

- Calculus Basics: Gradient, Hessian matrix, Taylor-approximation
- Convexity: Def. convex Set, Def. convex function, convex hull (smalles convex set that contains M)
- examples: norm(x), affine fct. f(x) = c.T x + b, quadratic fct. x.T A x
- equivalent: function convex, iff Epigraph is convex (see image)
- Jennsen-inequality
- convex <==> Hessian is positive semi-definite
- necessary condition: If $\hat{x}$ is local minimum of $f$, then $\nabla f(\hat{x}) = 0$
- sufficient condition: $\nabla f(\hat{x}) = 0$ and $H f(\hat{x})$ pos. definit, then local minimum 
- Optimization algorithms: iterative. Typees: gradient based, trust-region, 

